{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd64488a-6672-4765-b7ba-b23d67816445",
   "metadata": {},
   "source": [
    "## Demand Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7373240d-98e4-4ce4-ab80-2d56878d0bc4",
   "metadata": {},
   "source": [
    "### Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b261d5f1-7d3e-44ff-a239-6a50bbac3330",
   "metadata": {},
   "source": [
    "\n",
    "1. Plan an Approach - what steps to do you plan to follow and why? This can be high-level, but please include 1-2 paragraphs explaining your approach to a potential non-Data Scientist stakeholder.\n",
    "2. Forecast Generation - produce a 28 day forecast (i.e. for each item_id predict demand for days d_1942 thru d_1969)\n",
    "3. Forecast Evaluation - how will you evaluate your forecast peformance (how will this differ before/after going live)?\n",
    "4. Prepare for Discussion - your submission will be the starting point for a discussion, be prepared to talk about your solution, assumptions/tradeoffs, expect questions on how this exercise is analogous to data problems faced at Shipbob.\n",
    "5. Submit Response - See bottom section (either create git repo or email files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9ac16a-04a1-434b-9fbc-b2d97080c184",
   "metadata": {},
   "source": [
    "### PLAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019f4c4c-1d5a-4211-91af-0c9111aaad39",
   "metadata": {},
   "source": [
    "\n",
    "My typical approach would be:\n",
    "\n",
    "1. Understand data and how to access it. The aim is to forecast demand based on historical data.\n",
    "2. Exploration analysis: get an idea of data distribution, is there seasonalities? special days? some aspects that calls our attention?\n",
    "3. Try some simple model to  get a baseline. Then increasinlgy add complexity to the model and iterate until metrics are good enough. Typically\n",
    "   - xgboost, arima family, etc.\n",
    "\n",
    "Prepare data for training forecasting. Idea is to training and compare multiple models. \n",
    "\n",
    "Preprocessing ideas: \n",
    "\n",
    "    - Prepare lag features.\n",
    "    - Rolling features: e.g. moving average\n",
    "    - Prepare target, predict next day? next week? average demand for the future 28 days?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4f37354-fd6b-4852-b438-6bdb365162dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Additional packages installed, added to requirements\n",
    "# !pip install numpy=='1.26.4'\n",
    "# !pip install jupyter_black\n",
    "# !pip install category_encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f69b4be-8b05-4bc4-8502-c384004cee57",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./commons.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42c5e30a-50eb-498a-81b5-c72cea69a3f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1941 ['d_1', 'd_2', 'd_3', 'd_4', 'd_5'] ['d_1937', 'd_1938', 'd_1939', 'd_1940', 'd_1941']\n",
      "28 ['d_1942', 'd_1943', 'd_1944', 'd_1945', 'd_1946'] ['d_1965', 'd_1966', 'd_1967', 'd_1968', 'd_1969']\n",
      "Train Days: 1941 1941\n",
      "Test Days: 28 28\n"
     ]
    }
   ],
   "source": [
    "from utils import (\n",
    "    get_custom_calendar,\n",
    "    get_input_data,\n",
    "    prepare_datasets,\n",
    "    plot_actual_vs_pred,\n",
    "    compute_metrics,\n",
    "    build_model_name,\n",
    ")\n",
    "\n",
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "\n",
    "from process import *\n",
    "\n",
    "# 2013-09-25, 2016-05-22\n",
    "# NOTE: Last date is not included in time interval\n",
    "\n",
    "TRAIN_START = 1\n",
    "TRAIN_END = 1941\n",
    "TRAIN_START_DATE = datetime.datetime.strptime(\"2011-01-29\", \"%Y-%m-%d\")\n",
    "TRAIN_END_DATE = datetime.datetime.strptime(\"2016-05-23\", \"%Y-%m-%d\")\n",
    "\n",
    "\n",
    "TEST_START = 1942\n",
    "TEST_END = 1969\n",
    "TEST_START_DATE = datetime.datetime.strptime(\"2016-05-23\", \"%Y-%m-%d\")\n",
    "TEST_END_DATE = datetime.datetime.strptime(\"2016-06-20\", \"%Y-%m-%d\")\n",
    "\n",
    "\n",
    "train_cols = [f\"d_{i}\" for i in range(1, 1942)]\n",
    "test_cols = [f\"d_{i}\" for i in range(1942, 1970)]\n",
    "print(len(train_cols), train_cols[:5], train_cols[-5:])\n",
    "print(len(test_cols), test_cols[:5], test_cols[-5:])\n",
    "\n",
    "\n",
    "print(\"Train Days:\", (TRAIN_END_DATE - TRAIN_START_DATE).days, len(train_cols))\n",
    "print(\"Test Days:\", (TEST_END_DATE - TEST_START_DATE).days, len(test_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06296086-b71e-46d3-9025-bfeb7762ceec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-01-28 15:17:30.616\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mprocess\u001b[0m:\u001b[36mload_calendar\u001b[0m:\u001b[36m30\u001b[0m - \u001b[34m\u001b[1mBegin Loading Calendar Data...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1969, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>d</th>\n",
       "      <th>date</th>\n",
       "      <th>wm_yr_wk</th>\n",
       "      <th>year</th>\n",
       "      <th>wknu</th>\n",
       "      <th>event_name_1</th>\n",
       "      <th>event_type_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d_1</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>11101</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d_2</td>\n",
       "      <td>2011-01-30</td>\n",
       "      <td>11101</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>d_3</td>\n",
       "      <td>2011-01-31</td>\n",
       "      <td>11101</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d_4</td>\n",
       "      <td>2011-02-01</td>\n",
       "      <td>11101</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d_5</td>\n",
       "      <td>2011-02-02</td>\n",
       "      <td>11101</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1964</th>\n",
       "      <td>d_1965</td>\n",
       "      <td>2016-06-15</td>\n",
       "      <td>11620</td>\n",
       "      <td>16</td>\n",
       "      <td>20</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1965</th>\n",
       "      <td>d_1966</td>\n",
       "      <td>2016-06-16</td>\n",
       "      <td>11620</td>\n",
       "      <td>16</td>\n",
       "      <td>20</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1966</th>\n",
       "      <td>d_1967</td>\n",
       "      <td>2016-06-17</td>\n",
       "      <td>11620</td>\n",
       "      <td>16</td>\n",
       "      <td>20</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1967</th>\n",
       "      <td>d_1968</td>\n",
       "      <td>2016-06-18</td>\n",
       "      <td>11621</td>\n",
       "      <td>16</td>\n",
       "      <td>21</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1968</th>\n",
       "      <td>d_1969</td>\n",
       "      <td>2016-06-19</td>\n",
       "      <td>11621</td>\n",
       "      <td>16</td>\n",
       "      <td>21</td>\n",
       "      <td>NBAFinalsEnd</td>\n",
       "      <td>Sporting</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1969 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           d       date  wm_yr_wk  year  wknu  event_name_1 event_type_1\n",
       "0        d_1 2011-01-29     11101    11     1           nan          nan\n",
       "1        d_2 2011-01-30     11101    11     1           nan          nan\n",
       "2        d_3 2011-01-31     11101    11     1           nan          nan\n",
       "3        d_4 2011-02-01     11101    11     1           nan          nan\n",
       "4        d_5 2011-02-02     11101    11     1           nan          nan\n",
       "...      ...        ...       ...   ...   ...           ...          ...\n",
       "1964  d_1965 2016-06-15     11620    16    20           nan          nan\n",
       "1965  d_1966 2016-06-16     11620    16    20           nan          nan\n",
       "1966  d_1967 2016-06-17     11620    16    20           nan          nan\n",
       "1967  d_1968 2016-06-18     11621    16    21           nan          nan\n",
       "1968  d_1969 2016-06-19     11621    16    21  NBAFinalsEnd     Sporting\n",
       "\n",
       "[1969 rows x 7 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calendar = get_custom_calendar()\n",
    "print(calendar.shape)\n",
    "calendar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a25d3999-6aed-4a67-ac50-703ac12c7643",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "train_state = \"WI\"\n",
    "train_selected_item = \"FOODS_3_823\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54862c86-13bc-4d7b-901a-204b2a31a4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start date used for training: 2013-05-24\n",
      "For item: FOODS_3_823\n",
      "For state: WI\n"
     ]
    }
   ],
   "source": [
    "start_date = (TRAIN_END_DATE - datetime.timedelta(days=365 * 3)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "\n",
    "print(f\"Start date used for training: {start_date}\")\n",
    "print(f\"For item: {train_selected_item}\")\n",
    "print(f\"For state: {train_state}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac418d6f-37ad-41d9-91cd-660c88a9776d",
   "metadata": {},
   "source": [
    "## 0. Prepare input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a45dbb46-94c1-4e58-a7fd-df440b6c2590",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-01-28 15:17:36.649\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mprocess\u001b[0m:\u001b[36mload_prices\u001b[0m:\u001b[36m45\u001b[0m - \u001b[34m\u001b[1mBegin Loading Price Data...\u001b[0m\n",
      "\u001b[32m2025-01-28 15:17:38.135\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mprocess\u001b[0m:\u001b[36mload_sales\u001b[0m:\u001b[36m62\u001b[0m - \u001b[34m\u001b[1mBegin Loading Sales Data...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# id_cols = [\"id\", \"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"]\n",
    "prices = load_prices(PATH_INPUT)\n",
    "data = load_sales(PATH_INPUT, prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eaa23ce-54cb-4042-b28d-ded1fdd52a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTES: custom functions ensures data is sorted\n",
    "data = get_input_data(\n",
    "    data=data,\n",
    "    prices=prices,\n",
    "    calendar=calendar,\n",
    "    # start_date=start_date,\n",
    "    state_id=train_state,\n",
    "    item_id=train_selected_item,\n",
    "    drop_columns=[],\n",
    ")\n",
    "print(data.shape)\n",
    "data.dtypes\n",
    "# CA, (7764, 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5e374b-8d9e-4708-8d49-de3d8701def2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2649046d-6059-4162-911f-0793cdec37c4",
   "metadata": {},
   "source": [
    "## 1. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86bf611-e6b7-4073-b03a-231a54562de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def classify_columns(df):\n",
    "    ds = df.dtypes.reset_index().assign(Dtype=lambda x: x[0].astype(str))\n",
    "    features_types = ds.groupby(\"Dtype\")[\"index\"].apply(list).to_dict()\n",
    "    features_types[\"num\"] = []\n",
    "    for k, v in features_types.items():\n",
    "        if \"float\" in k or \"int\" in k:\n",
    "            features_types[\"num\"] += v\n",
    "    return features_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72edfd7-e0e8-4c4b-8b1f-7ec524d58b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original data shape:\", data.shape)\n",
    "\n",
    "df = prepare_datasets(data)\n",
    "df = df.dropna()\n",
    "\n",
    "print(\"Clean data shape:\", df.shape)\n",
    "df.iloc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a54096-3af4-48e6-b346-7b9a1a2336d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "idcols = [\"id\", \"item_id\", \"dept_id\", \"cat_id\", \"date\", \"d\", \"wm_yr_wk\"]\n",
    "target = [\"sales\"]\n",
    "features = [c for c in df.columns if c not in idcols + target]\n",
    "features_types = classify_columns(df[features])\n",
    "\n",
    "categorical_cols = features_types[\"category\"]\n",
    "numerical_cols = features_types[\"num\"]\n",
    "\n",
    "print(f\"Categorical cols: {categorical_cols}\")\n",
    "print(f\"Numerical cols: {numerical_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad978335-6727-45a7-9ce1-e6cfefbec179",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_train_test_split(X, y, test_size):\n",
    "    \"\"\"\n",
    "    Perform train-test split keeping time series sorted\n",
    "    \"\"\"\n",
    "    X_train = X.iloc[:-test_size]\n",
    "    y_train = y.iloc[:-test_size]\n",
    "    X_test = X.iloc[-test_size:]\n",
    "    y_test = y.iloc[-test_size:]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = custom_train_test_split(\n",
    "    X=df.dropna().drop(columns=[\"sales\"]), y=df.dropna()[\"sales\"], test_size=int(0.3 * len(df))\n",
    ")\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0174d25-d69e-4900-a568-baacce27a472",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448049e4-e41e-44a4-ac57-cbb92ef50431",
   "metadata": {},
   "source": [
    "## 2. Model Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651f16f8-d74c-4b58-a643-112c36481a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from category_encoders import CatBoostEncoder\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "def create_pipeline(numerical_columns: List[str], categorical_columns: List[str]):\n",
    "    # Preprocessing:\n",
    "    # Handle missing categorical values\n",
    "    # Use CatBoostEncoder\n",
    "    categorical_preprocessor = Pipeline(\n",
    "        steps=[\n",
    "            (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "            (\"encoder\", CatBoostEncoder()),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    numerical_preprocessor = Pipeline(\n",
    "        steps=[\n",
    "            (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Combine preprocessors in a ColumnTransformer\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numerical_preprocessor, numerical_columns),\n",
    "            (\"cat\", categorical_preprocessor, categorical_columns),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    rf_reg_params = {\n",
    "        \"bootstrap\": True,\n",
    "        \"ccp_alpha\": 0.0,\n",
    "        \"criterion\": \"squared_error\",\n",
    "        \"max_depth\": None,\n",
    "        \"max_features\": 1.0,\n",
    "        \"max_leaf_nodes\": None,\n",
    "        \"max_samples\": None,\n",
    "        \"min_impurity_decrease\": 0.0,\n",
    "        \"min_samples_leaf\": 1,\n",
    "        \"min_samples_split\": 2,\n",
    "        \"min_weight_fraction_leaf\": 0.0,\n",
    "        \"monotonic_cst\": None,\n",
    "        \"n_estimators\": 150,\n",
    "        \"n_jobs\": None,\n",
    "        \"oob_score\": False,\n",
    "        \"random_state\": 42,\n",
    "        \"verbose\": 0,\n",
    "        \"warm_start\": False,\n",
    "    }\n",
    "\n",
    "    pipeline = Pipeline(\n",
    "        steps=[\n",
    "            (\"preprocessor\", preprocessor),\n",
    "            # (\"to_numpy\", FunctionTransformer(lambda x: x.values)),  # Explicitly convert to NumPy array\n",
    "            (\"regressor\", RandomForestRegressor(**rf_reg_params)),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e65598-c96a-4de0-911c-4a924162b1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = create_pipeline(\n",
    "    categorical_columns=categorical_cols,\n",
    "    numerical_columns=numerical_cols,\n",
    ")\n",
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a639892-3737-4359-969f-e53d278cbb89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a22bec-38c4-4e53-97d8-2ce31db7769e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92285a7a-30e5-40fb-b942-afdbaecb58a6",
   "metadata": {},
   "source": [
    "### 2.1 Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14902ad0-0b29-4f60-a3da-66257a96c378",
   "metadata": {},
   "outputs": [],
   "source": [
    "fimp = pd.DataFrame(\n",
    "    dict(\n",
    "        features=[c for c in X_train.columns if c not in id_cols + [\"wm_yr_wk\"]],\n",
    "        importance=model.steps[1][1].feature_importances_,\n",
    "    )\n",
    ").sort_values(\"importance\", ascending=False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "_ = sns.barplot(fimp, y=\"features\", x=\"importance\", ax=ax, orient=\"h\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80478712-6f20-4fb6-9c88-623a660ed169",
   "metadata": {},
   "source": [
    "## 3. Model Assesment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699f5826-0de0-405e-abf1-a04ebb2d1fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_transform = model.steps[0][1].transform(X_train)\n",
    "# just checking\n",
    "# todo: we have to drop all those columns with std=0\n",
    "pd.DataFrame(x_transform).describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c7aefb-0c08-4941-be98-8838fc3dff76",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=2, figsize=(12, 5))\n",
    "\n",
    "# Train scores\n",
    "y_pred = model.predict(X_train)\n",
    "ax = plot_actual_vs_pred(y_train, y_pred, ax=axs[0])\n",
    "ax.set_title(\"Train\")\n",
    "metrics_train = compute_metrics(y_train, y_pred)\n",
    "\n",
    "# Test Metrics\n",
    "y_test_pred = model.predict(X_test)\n",
    "ax = plot_actual_vs_pred(y_test, y_test_pred, ax=axs[1])\n",
    "ax.set_title(\"Test\")\n",
    "metrics_test = compute_metrics(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46d5bc1-9000-4c93-95df-1762e83885dc",
   "metadata": {},
   "source": [
    "**NOTES**\n",
    "\n",
    "- Predictions are always below the max value of actual values saw durint training. This is expected due to characteristics of xgboost (tree based model).\n",
    "- Predictions are always below the actual values meaning this model is understimated demand.\n",
    "- Some improvements can be obtained if we optimized the metaparameters of the xgboost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55c2bbf-0df9-4d1c-9a0c-16ec3d1947b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\"train\": metrics_train, \"test\": metrics_test}).T.round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc08a7a7-47a6-49f7-987b-188b63f54266",
   "metadata": {},
   "source": [
    "**NOTES**\n",
    "- Clearly the model is overfitted. Results are much worst in the test scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16842f10-9553-47a6-88c0-f9837b9ad630",
   "metadata": {},
   "source": [
    "- First approach: Want to predict for 1 item in 1 store "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d303ff-ef05-4d88-96b4-94bccea79514",
   "metadata": {},
   "source": [
    "## 4. Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57ec748-f61c-44ee-8edb-c063ad9927bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_state, train_selected_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154753c9-2429-47cb-8d03-f216efd8a760",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = build_model_name(state_id=train_state, item_tag=train_selected_item.split(\"_\")[0])\n",
    "print(model_name)\n",
    "joblib.dump(model, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c235f9d3-51b2-4f1e-b80f-2d0e69ddd8cc",
   "metadata": {},
   "source": [
    "\n",
    "## 5. How To Continue... \n",
    "\n",
    "There are still many points to explore and improve:\n",
    "\n",
    "\n",
    "1. Optimization of Hyperparameters: Implement GridSearch to tune the metaparameters of the Random Forest model.\n",
    "2. Extend to Weekly/Monthly Demand Prediction: Adapt the framework to predict expected demand on a weekly or monthly basis.\n",
    "3. Predict Demand for Multiple Items: Explore alternatives for predicting sales across all items, either by parallelizing the existing framework or retraining the model with all items for a given store or state.\n",
    "5. Alternative Models: Explore the use of ARIMA for time series forecasting, as well as conduct analysis of seasonality and trends.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1acd383-6798-4397-a3df-a5f32a8cd8d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "retail-demand-forecast",
   "language": "python",
   "name": "retail_demand_forecast_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
